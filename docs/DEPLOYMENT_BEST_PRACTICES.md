# AiDesign éƒ¨ç½²æœ€ä½³å®è·µ

## ğŸ“‹ ç›®å½•

- [éƒ¨ç½²æ¶æ„](#éƒ¨ç½²æ¶æ„)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [Dockeréƒ¨ç½²](#dockeréƒ¨ç½²)
- [Kuberneteséƒ¨ç½²](#kuberneteséƒ¨ç½²)
- [CI/CDæµæ°´çº¿](#cicdæµæ°´çº¿)
- [å®‰å…¨é…ç½®](#å®‰å…¨é…ç½®)
- [ç›‘æ§å’Œæ—¥å¿—](#ç›‘æ§å’Œæ—¥å¿—)
- [å¤‡ä»½å’Œæ¢å¤](#å¤‡ä»½å’Œæ¢å¤)
- [æ•…éšœå¤„ç†](#æ•…éšœå¤„ç†)

## ğŸ—ï¸ éƒ¨ç½²æ¶æ„

### æ¨èæ¶æ„è®¾è®¡

#### ç”Ÿäº§ç¯å¢ƒæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è´Ÿè½½å‡è¡¡å±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    Nginx    â”‚  â”‚   Cloudflare â”‚  â”‚    DNS      â”‚ â”‚
â”‚  â”‚  (ä¸»è´Ÿè½½)   â”‚  â”‚   (CDN/é˜²æŠ¤) â”‚  â”‚   (è§£æ)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åº”ç”¨å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Frontend   â”‚  â”‚   Backend    â”‚  â”‚   AIæœåŠ¡    â”‚ â”‚
â”‚  â”‚   (React)   â”‚  â”‚  (Node.js)   â”‚  â”‚ (PyTorch)   â”‚ â”‚
â”‚  â”‚  å¤šå®ä¾‹     â”‚  â”‚   å¤šå®ä¾‹     â”‚  â”‚    GPU      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   æ•°æ®å±‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PostgreSQL  â”‚  â”‚    Redis     â”‚  â”‚  File Storageâ”‚ â”‚
â”‚  â”‚   (ä¸»ä»)    â”‚  â”‚   (é›†ç¾¤)     â”‚  â”‚   (åˆ†å¸ƒå¼)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### é«˜å¯ç”¨æ¶æ„ç»„ä»¶

**1. è´Ÿè½½å‡è¡¡**
- **ä¸»è´Ÿè½½å‡è¡¡å™¨**ï¼šNginx + Keepalived
- **CDNåŠ é€Ÿ**ï¼šCloudflare/AWS CloudFront
- **å¥åº·æ£€æŸ¥**ï¼šè‡ªåŠ¨æ£€æµ‹å’Œå‰”é™¤æ•…éšœèŠ‚ç‚¹
- **SSLç»ˆæ­¢**ï¼šç»Ÿä¸€SSLè¯ä¹¦ç®¡ç†

**2. åº”ç”¨æœåŠ¡**
- **å¤šå®ä¾‹éƒ¨ç½²**ï¼šè‡³å°‘2ä¸ªå®ä¾‹
- **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šåŸºäºè´Ÿè½½è‡ªåŠ¨è°ƒèŠ‚
- **æœåŠ¡å‘ç°**ï¼šConsul/Etcd
- **é…ç½®ç®¡ç†**ï¼šé›†ä¸­é…ç½®ä¸­å¿ƒ

**3. æ•°æ®å­˜å‚¨**
- **æ•°æ®åº“**ï¼šPostgreSQLä¸»ä»å¤åˆ¶
- **ç¼“å­˜**ï¼šRedis Cluster
- **æ–‡ä»¶å­˜å‚¨**ï¼šMinIO/S3å…¼å®¹å­˜å‚¨
- **å¤‡ä»½**ï¼šå®šæœŸå…¨é‡+å¢é‡å¤‡ä»½

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### ç¡¬ä»¶è¦æ±‚

#### æœ€å°é…ç½®ï¼ˆå¼€å‘/æµ‹è¯•ï¼‰

| ç»„ä»¶ | CPU | å†…å­˜ | å­˜å‚¨ | ç½‘ç»œ |
|------|-----|------|------|------|
| åº”ç”¨æœåŠ¡å™¨ | 2æ ¸ | 4GB | 50GB SSD | 100Mbps |
| æ•°æ®åº“æœåŠ¡å™¨ | 2æ ¸ | 4GB | 100GB SSD | 100Mbps |
| RedisæœåŠ¡å™¨ | 1æ ¸ | 2GB | 20GB SSD | 100Mbps |

#### æ¨èé…ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

| ç»„ä»¶ | CPU | å†…å­˜ | å­˜å‚¨ | ç½‘ç»œ |
|------|-----|------|------|------|
| åº”ç”¨æœåŠ¡å™¨ | 4æ ¸ | 8GB | 100GB NVMe | 1Gbps |
| æ•°æ®åº“æœåŠ¡å™¨ | 8æ ¸ | 16GB | 500GB NVMe | 1Gbps |
| RedisæœåŠ¡å™¨ | 4æ ¸ | 8GB | 100GB NVMe | 1Gbps |
| AIæœåŠ¡å™¨ | 16æ ¸ | 64GB | 1TB NVMe | 10Gbps |

#### ä¼ä¸šé…ç½®ï¼ˆé«˜è´Ÿè½½ï¼‰

| ç»„ä»¶ | CPU | å†…å­˜ | å­˜å‚¨ | ç½‘ç»œ | GPU |
|------|-----|------|------|------|-----|
| åº”ç”¨æœåŠ¡å™¨é›†ç¾¤ | 8æ ¸ | 16GB | 200GB NVMe | 10Gbps | - |
| æ•°æ®åº“é›†ç¾¤ | 16æ ¸ | 64GB | 2TB NVMe | 10Gbps | - |
| ç¼“å­˜é›†ç¾¤ | 8æ ¸ | 32GB | 500GB NVMe | 10Gbps | - |
| AIæœåŠ¡å™¨é›†ç¾¤ | 32æ ¸ | 128GB | 2TB NVMe | 40Gbps | 4x A100 |

### æ“ä½œç³»ç»Ÿè¦æ±‚

#### æ¨èæ“ä½œç³»ç»Ÿ

**Linuxå‘è¡Œç‰ˆ**
```bash
# Ubuntu 20.04/22.04 LTS
apt update && apt install -y \
    curl wget gnupg2 software-properties-common \
    apt-transport-https ca-certificates \
    lsb-release

# CentOS/RHEL 8/9
yum update -y && yum groupinstall -y "Development Tools"
yum install -y curl wget htop iotop

# Dockerå®‰è£…
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh
```

**å†…æ ¸å‚æ•°ä¼˜åŒ–**
```bash
# /etc/sysctl.d/99-aiplatform.conf
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
fs.file-max = 2097152
vm.swappiness = 10

# åº”ç”¨é…ç½®
sysctl --system
```

## ğŸ³ Dockeréƒ¨ç½²

### ç”Ÿäº§ç¯å¢ƒDockeré…ç½®

#### Docker Composeä¼˜åŒ–

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # Nginxè´Ÿè½½å‡è¡¡
  nginx:
    image: nginx:alpine
    container_name: aideign_nginx_lb
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/ssl:ro
      - nginx_logs:/var/log/nginx
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - frontend
      - backend
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
    networks:
      - aiplatform_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # å‰ç«¯åº”ç”¨
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    image: aiplatform/frontend:latest
    container_name: aideign_frontend_1
    environment:
      - NODE_ENV=production
      - API_URL=https://api.aiplatform.com
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    networks:
      - aiplatform_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3

  # åç«¯åº”ç”¨
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    image: aiplatform/backend:latest
    container_name: aideign_backend_1
    environment:
      - NODE_ENV=production
      - POSTGRES_HOST=postgres
      - REDIS_HOST=redis
      - JWT_SECRET=${JWT_SECRET}
      - AI_MODEL_PATH=/models
    volumes:
      - models_data:/models:ro
      - logs_data:/app/logs
      - uploads_data:/app/uploads
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    networks:
      - aiplatform_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # AIæœåŠ¡ï¼ˆå¯é€‰ï¼‰
  ai-service:
    build:
      context: ./ai-service
      dockerfile: Dockerfile.prod
    image: aiplatform/ai-service:latest
    container_name: aideign_ai_service_1
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - MODEL_CACHE_SIZE=4GB
    volumes:
      - models_data:/models:ro
      - gpu_cache:/tmp/gpu_cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
    devices:
      - /dev/nvidia0
      - /dev/nvidia1
    networks:
      - aiplatform_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  models_data:
    driver: local
  uploads_data:
    driver: local
  logs_data:
    driver: local
  nginx_logs:
    driver: local
  gpu_cache:
    driver: local

networks:
  aiplatform_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
```

#### ç”Ÿäº§ç¯å¢ƒDockerfile

**å‰ç«¯Dockerfile.prod**
```dockerfile
# å¤šé˜¶æ®µæ„å»ºä¼˜åŒ–
FROM node:18-alpine AS builder

WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

# å¤åˆ¶æºä»£ç 
COPY . .

# æ„å»ºåº”ç”¨
RUN npm run build

# ç”Ÿäº§é•œåƒ
FROM nginx:alpine

# å®‰è£…å¿…è¦å·¥å…·
RUN apk add --no-cache curl

# å¤åˆ¶æ„å»ºäº§ç‰©
COPY --from=builder /app/dist /usr/share/nginx/html

# å¤åˆ¶Nginxé…ç½®
COPY nginx/nginx.prod.conf /etc/nginx/conf.d/default.conf

# åˆ›å»ºérootç”¨æˆ·
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nextjs -u 1001

# è®¾ç½®æƒé™
RUN chown -R nextjs:nodejs /usr/share/nginx/html && \
    chown -R nextjs:nodejs /var/cache/nginx && \
    chown -R nextjs:nodejs /var/log/nginx

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER nextjs

EXPOSE 8080

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080 || exit 1

CMD ["nginx", "-g", "daemon off;"]
```

**åç«¯Dockerfile.prod**
```dockerfile
FROM node:18-alpine AS deps

WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

# ç”Ÿäº§ä¾èµ–
FROM node:18-alpine AS runtime

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apk add --no-cache \
    curl \
    dumb-init \
    ca-certificates

WORKDIR /app

# å¤åˆ¶node_modules
COPY --from=deps /app/node_modules ./node_modules

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=nodejs:nodejs . .

# åˆ›å»ºæ—¥å¿—ç›®å½•
RUN mkdir -p /app/logs && chown nodejs:nodejs /app/logs

# åˆ›å»ºérootç”¨æˆ·
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

USER nodejs

EXPOSE 3000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:3000/health || exit 1

# ä½¿ç”¨dumb-initä½œä¸ºPID 1
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "src/app.js"]
```

### éƒ¨ç½²è„šæœ¬

#### è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é…ç½®å˜é‡
APP_NAME="aiplatform"
DEPLOY_DIR="/opt/aiplatform"
BACKUP_DIR="/opt/backups"
LOG_FILE="/var/log/deploy.log"
COMPOSE_FILE="docker-compose.prod.yml"

# æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# é”™è¯¯å¤„ç†
error_exit() {
    log "ERROR: $1"
    exit 1
}

# æ£€æŸ¥ç¯å¢ƒ
check_environment() {
    log "æ£€æŸ¥éƒ¨ç½²ç¯å¢ƒ..."
    
    # æ£€æŸ¥Docker
    if ! command -v docker &> /dev/null; then
        error_exit "Dockeræœªå®‰è£…"
    fi
    
    # æ£€æŸ¥Docker Compose
    if ! command -v docker-compose &> /dev/null; then
        error_exit "Docker Composeæœªå®‰è£…"
    fi
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    available_space=$(df /opt | awk 'NR==2 {print $4}')
    if [ $available_space -lt 10485760 ]; then  # 10GB
        error_exit "ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œè‡³å°‘éœ€è¦10GB"
    fi
    
    log "ç¯å¢ƒæ£€æŸ¥é€šè¿‡"
}

# å¤‡ä»½å½“å‰ç‰ˆæœ¬
backup_current() {
    log "å¤‡ä»½å½“å‰éƒ¨ç½²..."
    
    backup_time=$(date +%Y%m%d_%H%M%S)
    backup_dir="$BACKUP_DIR/$backup_time"
    
    mkdir -p "$backup_dir"
    
    # å¤‡ä»½æ•°æ®åº“
    docker exec aideign_postgres pg_dump -U postgres aiplatform > "$backup_dir/database.sql"
    
    # å¤‡ä»½ä¸Šä¼ æ–‡ä»¶
    docker run --rm -v aideign_uploads_data:/data -v "$backup_dir":/backup \
        alpine tar czf /backup/uploads.tar.gz -C /data .
    
    # å¤‡ä»½é…ç½®æ–‡ä»¶
    cp -r $DEPLOY_DIR/.env* "$backup_dir/"
    
    log "å¤‡ä»½å®Œæˆ: $backup_dir"
}

# æ‹‰å–æœ€æ–°ä»£ç 
pull_latest_code() {
    log "æ‹‰å–æœ€æ–°ä»£ç ..."
    
    cd $DEPLOY_DIR
    git fetch origin
    git reset --hard origin/main
    
    log "ä»£ç æ›´æ–°å®Œæˆ"
}

# æ„å»ºå’Œéƒ¨ç½²
deploy_app() {
    log "å¼€å§‹éƒ¨ç½²åº”ç”¨..."
    
    # åœæ­¢æ—§æœåŠ¡
    cd $DEPLOY_DIR
    docker-compose -f $COMPOSE_FILE down
    
    # æ‹‰å–æœ€æ–°é•œåƒ
    docker-compose -f $COMPOSE_FILE pull
    
    # æ„å»ºæ–°é•œåƒ
    docker-compose -f $COMPOSE_FILE build --no-cache
    
    # å¯åŠ¨æœåŠ¡
    docker-compose -f $COMPOSE_FILE up -d
    
    log "æœåŠ¡å¯åŠ¨å®Œæˆ"
}

# å¥åº·æ£€æŸ¥
health_check() {
    log "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    sleep 30
    
    # æ£€æŸ¥å„ä¸ªæœåŠ¡
    services=("nginx" "frontend" "backend" "postgres" "redis")
    
    for service in "${services[@]}"; do
        if docker exec aideign_$service curl -f http://localhost/health > /dev/null 2>&1; then
            log "âœ“ $service å¥åº·æ£€æŸ¥é€šè¿‡"
        else
            error_exit "$service å¥åº·æ£€æŸ¥å¤±è´¥"
        fi
    done
    
    log "æ‰€æœ‰æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡"
}

# æ¸…ç†æ—§é•œåƒ
cleanup() {
    log "æ¸…ç†æ—§é•œåƒ..."
    
    # åˆ é™¤æœªä½¿ç”¨çš„é•œåƒ
    docker image prune -f
    
    # åˆ é™¤æ—§ç‰ˆæœ¬é•œåƒï¼ˆä¿ç•™æœ€è¿‘3ä¸ªç‰ˆæœ¬ï¼‰
    docker images --format "table {{.Repository}}\t{{.Tag}}\t{{.ID}}" | \
        grep aiplatform | tail -n +4 | \
        awk '{print $3}' | xargs -r docker rmi -f
    
    log "æ¸…ç†å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    log "å¼€å§‹éƒ¨ç½² $APP_NAME..."
    
    check_environment
    backup_current
    pull_latest_code
    deploy_app
    health_check
    cleanup
    
    log "éƒ¨ç½²å®Œæˆï¼"
    
    # å‘é€éƒ¨ç½²é€šçŸ¥
    send_notification "AiDesignéƒ¨ç½²æˆåŠŸ" "æ‰€æœ‰æœåŠ¡æ­£å¸¸è¿è¡Œ"
}

# å‘é€é€šçŸ¥
send_notification() {
    local title="$1"
    local message="$2"
    
    # é’‰é’‰é€šçŸ¥
    if [ -n "$DINGTALK_WEBHOOK" ]; then
        curl -H "Content-Type: application/json" \
             -d "{\"msgtype\": \"text\", \"text\": \"$title\n$message\"}" \
             "$DINGTALK_WEBHOOK"
    fi
    
    # é‚®ä»¶é€šçŸ¥
    if [ -n "$DEPLOY_EMAIL" ]; then
        echo "$message" | mail -s "$title" "$DEPLOY_EMAIL"
    fi
}

# æ‰§è¡Œéƒ¨ç½²
main "$@"
```

## â˜¸ï¸ Kuberneteséƒ¨ç½²

### Kubernetesé…ç½®

#### é›†ç¾¤æ¶æ„è®¾è®¡

```yaml
# kubernetes/namespace.yml
apiVersion: v1
kind: Namespace
metadata:
  name: aiplatform
  labels:
    name: aiplatform
    environment: production

---
# kubernetes/configmap.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: aiplatform-config
  namespace: aiplatform
data:
  NODE_ENV: "production"
  LOG_LEVEL: "info"
  API_URL: "https://api.aiplatform.com"
  REDIS_HOST: "redis-service"
  POSTGRES_HOST: "postgres-service"
```

#### åº”ç”¨éƒ¨ç½²é…ç½®

```yaml
# kubernetes/backend-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-deployment
  namespace: aiplatform
  labels:
    app: backend
    environment: production
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
        environment: production
    spec:
      containers:
      - name: backend
        image: aiplatform/backend:latest
        ports:
        - containerPort: 3000
        env:
        - name: NODE_ENV
          valueFrom:
            configMapKeyRef:
              name: aiplatform-config
              key: NODE_ENV
        - name: POSTGRES_HOST
          valueFrom:
            configMapKeyRef:
              name: aiplatform-config
              key: POSTGRES_HOST
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: aiplatform-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: aiplatform-secrets
              key: redis-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "curl -X POST http://localhost:3000/shutdown"]

---
# kubernetes/backend-service.yml
apiVersion: v1
kind: Service
metadata:
  name: backend-service
  namespace: aiplatform
  labels:
    app: backend
spec:
  selector:
    app: backend
  ports:
  - name: http
    port: 80
    targetPort: 3000
  type: ClusterIP
```

#### Ingressé…ç½®

```yaml
# kubernetes/ingress.yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: aiplatform-ingress
  namespace: aiplatform
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
spec:
  tls:
  - hosts:
    - api.aiplatform.com
    - app.aiplatform.com
    secretName: aiplatform-tls
  rules:
  - host: api.aiplatform.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: backend-service
            port:
              number: 80
  - host: app.aiplatform.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

#### è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®

```yaml
# kubernetes/hpa.yml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
  namespace: aiplatform
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend-deployment
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
      selectPolicy: Max
```

## ğŸ”„ CI/CDæµæ°´çº¿

### GitLab CIé…ç½®

```yaml
# .gitlab-ci.yml
stages:
  - test
  - build
  - deploy-staging
  - deploy-production

variables:
  DOCKER_REGISTRY: registry.gitlab.com/aiplatform
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  MAVEN_OPTS: "-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository"

# æµ‹è¯•é˜¶æ®µ
test:backend:
  stage: test
  image: node:18-alpine
  services:
    - postgres:13
    - redis:6-alpine
  variables:
    POSTGRES_DB: test_db
    POSTGRES_USER: test_user
    POSTGRES_PASSWORD: test_pass
    POSTGRES_HOST: postgres
    REDIS_URL: redis://redis:6379
  before_script:
    - cd backend
    - npm ci
  script:
    - npm run lint
    - npm run test:unit
    - npm run test:integration
  coverage: '/Coverage: \d+\.\d+%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml
    paths:
      - coverage/
    expire_in: 1 week
  only:
    - merge_requests
    - main
    - develop

test:frontend:
  stage: test
  image: node:18-alpine
  before_script:
    - cd frontend
    - npm ci
  script:
    - npm run lint
    - npm run test:unit
    - npm run test:e2e
  coverage: '/Coverage: \d+\.\d+%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml
    paths:
      - coverage/
    expire_in: 1 week
  only:
    - merge_requests
    - main
    - develop

# æ„å»ºé˜¶æ®µ
build:backend:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  variables:
    IMAGE_TAG: $CI_COMMIT_SHORT_SHA
  script:
    - cd backend
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build -f Dockerfile.prod -t $DOCKER_REGISTRY/backend:$IMAGE_TAG .
    - docker push $DOCKER_REGISTRY/backend:$IMAGE_TAG
    - docker tag $DOCKER_REGISTRY/backend:$IMAGE_TAG $DOCKER_REGISTRY/backend:latest
    - docker push $DOCKER_REGISTRY/backend:latest
  only:
    - main
    - develop

build:frontend:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  variables:
    IMAGE_TAG: $CI_COMMIT_SHORT_SHA
  script:
    - cd frontend
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build -f Dockerfile.prod -t $DOCKER_REGISTRY/frontend:$IMAGE_TAG .
    - docker push $DOCKER_REGISTRY/frontend:$IMAGE_TAG
    - docker tag $DOCKER_REGISTRY/frontend:$IMAGE_TAG $DOCKER_REGISTRY/frontend:latest
    - docker push $DOCKER_REGISTRY/frontend:latest
  only:
    - main
    - develop

# éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
deploy:staging:
  stage: deploy-staging
  image: bitnami/kubectl:latest
  environment:
    KUBECONFIG: $STAGING_KUBECONFIG
  script:
    - kubectl set image deployment/backend-deployment backend=$DOCKER_REGISTRY/backend:$CI_COMMIT_SHORT_SHA
    - kubectl set image deployment/frontend-deployment frontend=$DOCKER_REGISTRY/frontend:$CI_COMMIT_SHORT_SHA
    - kubectl rollout status deployment/backend-deployment
    - kubectl rollout status deployment/frontend-deployment
  only:
    - develop

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy:production:
  stage: deploy-production
  image: bitnami/kubectl:latest
  environment:
    KUBECONFIG: $PRODUCTION_KUBECONFIG
  script:
    - echo "éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
    - kubectl set image deployment/backend-deployment backend=$DOCKER_REGISTRY/backend:$CI_COMMIT_SHORT_SHA
    - kubectl set image deployment/frontend-deployment frontend=$DOCKER_REGISTRY/frontend:$CI_COMMIT_SHORT_SHA
    - kubectl rollout status deployment/backend-deployment
    - kubectl rollout status deployment/frontend-deployment
    - kubectl get pods -n aiplatform
  when: manual
  only:
    - main
  allow_failure: false
```

### GitHub Actionsé…ç½®

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [18.x]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
    
    - name: Install dependencies
      run: |
        cd backend && npm ci
        cd ../frontend && npm ci
    
    - name: Run tests
      run: |
        cd backend && npm run test
        cd ../frontend && npm run test
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        directory: ./coverage
        files: ./coverage/cobertura-coverage.xml

  build-and-deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
    
    - name: Build and push Docker images
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        ECR_REPOSITORY: aiplatform
      run: |
        # Build backend
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY/backend:$GITHUB_SHA ./backend
        docker push $ECR_REGISTRY/$ECR_REPOSITORY/backend:$GITHUB_SHA
        
        # Build frontend
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY/frontend:$GITHUB_SHA ./frontend
        docker push $ECR_REGISTRY/$ECR_REPOSITORY/frontend:$GITHUB_SHA
    
    - name: Deploy to Kubernetes
      run: |
        aws eks update-kubeconfig --name aiplatform-cluster
        kubectl set image deployment/backend-deployment backend=$ECR_REGISTRY/$ECR_REPOSITORY/backend:$GITHUB_SHA
        kubectl set image deployment/frontend-deployment frontend=$ECR_REGISTRY/$ECR_REPOSITORY/frontend:$GITHUB_SHA
        kubectl rollout status deployment/backend-deployment
        kubectl rollout status deployment/frontend-deployment
```

## ğŸ”’ å®‰å…¨é…ç½®

### SSL/TLSé…ç½®

#### Let's Encryptè¯ä¹¦é…ç½®

```bash
#!/bin/bash
# setup-ssl.sh - SSLè¯ä¹¦é…ç½®

DOMAIN="api.aiplatform.com"
EMAIL="admin@aiplatform.com"
NGINX_DIR="/etc/nginx"
CERT_DIR="$NGINX_DIR/ssl"

# å®‰è£…Certbot
apt update
apt install -y certbot python3-certbot-nginx

# ç”³è¯·è¯ä¹¦
certbot --nginx -d $DOMAIN -d www.$DOMAIN --email $EMAIL --agree-tos --no-eff-email -n

# é…ç½®è‡ªåŠ¨ç»­æœŸ
(crontab -l 2>/dev/null; echo "0 12 * * * /usr/bin/certbot renew --quiet --nginx") | crontab -

# è®¾ç½®å¼ºåŒ–çš„Nginx SSLé…ç½®
cat > $NGINX_DIR/conf.d/ssl.conf << EOF
ssl_protocols TLSv1.2 TLSv1.3;
ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
ssl_prefer_server_ciphers off;
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;
ssl_session_tickets off;

# HSTS
add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

# å…¶ä»–å®‰å…¨å¤´
add_header X-Frame-Options DENY;
add_header X-Content-Type-Options nosniff;
add_header X-XSS-Protection "1; mode=block";
add_header Referrer-Policy "strict-origin-when-cross-origin";
EOF

echo "SSLé…ç½®å®Œæˆ"
```

### é˜²ç«å¢™é…ç½®

```bash
#!/bin/bash
# setup-firewall.sh - é˜²ç«å¢™é…ç½®

# UFWåŸºæœ¬é…ç½®
ufw --force reset
ufw default deny incoming
ufw default allow outgoing

# å…è®¸SSHï¼ˆé™åˆ¶IPï¼‰
ufw allow from 192.168.1.0/24 to any port 22 proto tcp

# å…è®¸HTTP/HTTPS
ufw allow 80/tcp
ufw allow 443/tcp

# å…è®¸ç›‘æ§ç«¯å£
ufw allow from 10.0.0.0/8 to any port 3002 proto tcp

# å¯ç”¨é˜²ç«å¢™
ufw --force enable

# å®‰è£…fail2ban
apt install -y fail2ban

# é…ç½®SSHä¿æŠ¤
cat > /etc/fail2ban/jail.local << EOF
[DEFAULT]
bantime = 3600
findtime = 600
maxretry = 3

[sshd]
enabled = true
port = ssh
filter = sshd
logpath = /var/log/auth.log
maxretry = 3
EOF

# é…ç½®Nginxä¿æŠ¤
cat > /etc/fail2ban/jail.d/nginx.conf << EOF
[nginx-http-auth]
enabled = true
filter = nginx-http-auth
port = http,https
logpath = /var/log/nginx/error.log
maxretry = 5
findtime = 600
bantime = 3600
EOF

systemctl enable fail2ban
systemctl restart fail2ban

echo "é˜²ç«å¢™é…ç½®å®Œæˆ"
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### Prometheusç›‘æ§é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
    - role: endpoints
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
      action: keep
        regex: __meta_kubernetes_namespace;kubernetes-apiservers;__meta_kubernetes_service_name;kubernetes;__meta_kubernetes_endpoint_port_name;https

  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
    - role: node
    relabel_configs:
    - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
        replacement: $1
    - target_label: __address__
        replacement: kubernetes-default
    - source_labels: [__address__]
        regex: (.+)
        target_label: instance
        replacement: ${1}
```

### Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "id": null,
    "title": "AiDesign Production Dashboard",
    "tags": ["aiplatform", "production"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (service)",
            "legendFormat": "{{service}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0},
        "yAxes": [
          {
            "label": "Requests/sec"
          }
        ]
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "95th percentile"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

### æ—¥å¿—ç®¡ç†

```yaml
# docker-compose.logging.yml
version: '3.8'

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "100m"
    max-file: "3"
    labels: "service,environment"

services:
  # ELK Stack
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    logging: *default-logging

  logstash:
    image: docker.elastic.co/logstash/logstash:7.14.0
    volumes:
      - ./logging/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "5044:5044"
    depends_on:
      - elasticsearch
    logging: *default-logging

  kibana:
    image: docker.elastic.co/kibana/kibana:7.14.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    logging: *default-logging

volumes:
  elasticsearch_data:
```

## ğŸ’¾ å¤‡ä»½å’Œæ¢å¤

### æ•°æ®åº“å¤‡ä»½ç­–ç•¥

```bash
#!/bin/bash
# backup-database.sh - æ•°æ®åº“å¤‡ä»½è„šæœ¬

BACKUP_DIR="/opt/backups/database"
RETENTION_DAYS=30
DB_NAME="aiplatform"
DB_USER="postgres"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å…¨é‡å¤‡ä»½
full_backup() {
    local backup_file="$BACKUP_DIR/full_backup_$(date +%Y%m%d_%H%M%S).sql"
    
    echo "å¼€å§‹å…¨é‡å¤‡ä»½: $backup_file"
    
    docker exec aideign_postgres pg_dump -U $DB_USER -d $DB_NAME \
        --no-password --verbose --clean --if-exists \
        > $backup_file
    
    if [ $? -eq 0 ]; then
        echo "å…¨é‡å¤‡ä»½å®Œæˆ: $backup_file"
        compress_backup $backup_file
    else
        echo "å…¨é‡å¤‡ä»½å¤±è´¥"
        exit 1
    fi
}

# å¢é‡å¤‡ä»½
incremental_backup() {
    local backup_file="$BACKUP_DIR/incremental_backup_$(date +%Y%m%d_%H%M%S).sql"
    
    echo "å¼€å§‹å¢é‡å¤‡ä»½: $backup_file"
    
    # è·å–ä¸Šæ¬¡å¤‡ä»½æ—¶é—´æˆ³
    local last_backup=$(ls -t $BACKUP_DIR/full_backup_*.sql.gz 2>/dev/null | head -1 | xargs basename -s .sql.gz)
    
    if [ -n "$last_backup" ]; then
        # è¿™é‡Œåº”è¯¥å®ç°å¢é‡å¤‡ä»½é€»è¾‘
        echo "å¢é‡å¤‡ä»½åŸºäº: $last_backup"
    else
        # å¦‚æœæ²¡æœ‰å…¨é‡å¤‡ä»½ï¼Œæ‰§è¡Œå…¨é‡å¤‡ä»½
        full_backup
        return
    fi
}

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
compress_backup() {
    local file=$1
    echo "å‹ç¼©å¤‡ä»½æ–‡ä»¶: $file"
    gzip $file
    
    if [ $? -eq 0 ]; then
        echo "å‹ç¼©å®Œæˆ: $file.gz"
        rm $file
    else
        echo "å‹ç¼©å¤±è´¥: $file"
        exit 1
    fi
}

# æ¸…ç†è¿‡æœŸå¤‡ä»½
cleanup_old_backups() {
    echo "æ¸…ç† $RETENTION_DAYS å¤©å‰çš„å¤‡ä»½æ–‡ä»¶..."
    find $BACKUP_DIR -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete
    find $BACKUP_DIR -name "*.sql" -mtime +$RETENTION_DAYS -delete
}

# éªŒè¯å¤‡ä»½å®Œæ•´æ€§
verify_backup() {
    local file=$1
    echo "éªŒè¯å¤‡ä»½å®Œæ•´æ€§: $file"
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if [ ! -s "$file" ]; then
        echo "å¤‡ä»½æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨: $file"
        return 1
    fi
    
    # æ£€æŸ¥SQLè¯­æ³•
    if gunzip -t "$file" 2>/dev/null; then
        echo "å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡: $file"
        return 0
    else
        echo "å¤‡ä»½æ–‡ä»¶æŸå: $file"
        return 1
    fi
}

# ä¸»æ‰§è¡Œé€»è¾‘
case $1 in
    full)
        full_backup
        ;;
    incremental)
        incremental_backup
        ;;
    *)
        echo "ç”¨æ³•: $0 {full|incremental}"
        exit 1
        ;;
esac

cleanup_old_backups

echo "å¤‡ä»½æµç¨‹å®Œæˆ"
```

### è‡ªåŠ¨åŒ–å¤‡ä»½è°ƒåº¦

```yaml
# kubernetes/backup-cronjob.yml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: database-backup
  namespace: aiplatform
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:13-alpine
            command:
            - /bin/bash
            - -c
            - |
              BACKUP_FILE="/backup/backup_$(date +%Y%m%d_%H%M%S).sql"
              pg_dump -h $POSTGRES_HOST -U $POSTGRES_USER -d $POSTGRES_DB \
                  --no-password --verbose --clean --if-exists \
                  > $BACKUP_FILE
              
              if [ $? -eq 0 ]; then
                gzip $BACKUP_FILE
                echo "å¤‡ä»½æˆåŠŸ: $BACKUP_FILE.gz"
              else
                echo "å¤‡ä»½å¤±è´¥"
                exit 1
              fi
            env:
            - name: POSTGRES_HOST
              valueFrom:
                configMapKeyRef:
                  name: aiplatform-config
                  key: POSTGRES_HOST
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: aiplatform-secrets
                  key: postgres-user
            - name: POSTGRES_DB
              valueFrom:
                configMapKeyRef:
                  name: aiplatform-config
                  key: POSTGRES_DB
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
          backoffLimit: 3
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
```

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.1.0  
**æ›´æ–°æ—¶é—´**ï¼š2024å¹´1æœˆ  
**ç»´æŠ¤å›¢é˜Ÿ**ï¼šAiDesignè¿ç»´å›¢é˜Ÿ

éƒ¨ç½²æœ€ä½³å®è·µéœ€è¦æ ¹æ®å®é™…ç¯å¢ƒå’Œéœ€æ±‚ä¸æ–­è°ƒæ•´ï¼Œå»ºè®®å®šæœŸå®¡æŸ¥å’Œæ›´æ–°éƒ¨ç½²ç­–ç•¥ä»¥ç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå®‰å…¨æ€§ã€‚